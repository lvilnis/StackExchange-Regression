\documentclass[11pt]{article}

\usepackage{graphicx, amsmath, amsfonts, amssymb, microtype, fullpage, url, algorithm, algorithmic, hyperref, longtable}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\setlength{\parskip}{5pt}

\title{Project Report - Classification of questions on stackoverflow\\ Machine Learning, Fall 2012}
\author{Luke Vilnis, Annamalai Natarajan, Ariel Kobren}

\begin{document}
\sloppy

\maketitle

\section{Problem Statement}

``What is the best programming language?'' There are numerous answers
to this question but very little consensus among programmers as to the
correct answer. While this question is interesting and can be fun to
discuss, it will only clutter a forum designed for objective questions
answering. There are many other questions that have no place in
objective, topic-specific forums (e.g. question unrelated to the
subject matter discussed on the form). However, given a specific
question answering forum, it is quite difficult to discover content
that doesn't belong.

\noindent Stackoverflow \cite{website:stackoverflow}, a web forum
designed to host objective software and programming related question,
has this exact problem. On StackOverflow, any user may ask any
question; those questions are subsequently answered by other users on
the website. StackOverflow has little infrastructure to restrict users
from posting irrelevant and/or subjective questions on the site. To
keep the forum clean and interesting, the website moderators (and a
handful of influential users) have the ability to \emph{close} these
inappropriate questions. After a question is closed, it acquires a
closed tag, users cannot submit answers to it and it also is very
unlikely to top any search results over questions. While the question
is still reachable, these measures effectively remove the question
from public discussion.

\begin{figure}
\centering
\fbox{\includegraphics[width=4in,height=3.5in]{images/example_label.pdf}}
\caption{Sample closed question}
\label{fig:sample}
\end{figure}

\noindent Although StackOverflow has explicit policies regarding when
a question may be closed, these policies are largely
subjective. According to StackOverflow, a question may be closed if it
falls into one of these five categories:

\begin{itemize}
\item \textbf{Exact duplicate}: This category describes questions which may be reasonable and on-topic for StackOverflow, but have already appeared on the site.
\item \textbf{Off-topic}: Since StackOverflow is meant for software and programming related questions, questions that relate to domain-specific software packages or otherwise require the attention of specialists not likely to be found on StackOverflow are off-topic. An example of a question closed as off-topic is ``From the Brain imaging toolbox, AFNI, I see lots of activity in the Dorso Lateral pre-frontal cortex, does it mean that part of the brain is involved in activity A?''
\item \textbf{Not constructive}: Questions are closed as ``Not constructive'' when they solicit opinion, debate, arguments, polling, or extended discussion. Our opening question, ``What is the best programming language?'' is a common example.
\item \textbf{Not a real question}: When questions are ambiguous, vague, incomplete, overly broad or rhetorical. e.g. ``Sort HTML attributes with JavaScript'' - this user does not reveal the problem they encountered when attempting to perform this task.
\item \textbf{Too localized}: This comes up when the question pertains to a problem which occurs in very a specific setting and will not benefit future users of the forum. In general this is reserved for e.g. questions about very specific old versions of certain software and libraries.
\end{itemize}

\noindent Figure \ref{fig:sample} shows a sample of closed question. An important consequence of this categorization of closed questions is that there is nothing essentially wrong with questions that are closed as ``Exact duplicate,'' aside from the fact that they were asked at the wrong point in time.Elimination of duplicate questions from the training data prevents us from confusing the classifier, which is attempting to identify structral commonalities between closed questions.

\noindent Currently, closing questions is a manual process that
depends on the StackOverflow moderators paying close attention to every
question being asked. Closed questions are also not easy to spot:
Stackoverflow estimates that only 6\% of its questions are closed each
year.

\noindent In our project, we solve the problem of detecting StackOverflow
questions that should be closed.  Using data gathered directly from
StackOverflow and various machine learning techniques, we train
algorithms to perform this detection automatically.

\section{Data set}
In our initial work, we used a data set collected using
Stackoverflow's \emph{StackExchange Data Explorer}
\cite{website:stackexchange}. This online tool
allows users to query raw data directly from
Stackoverflow. Specifically, we issued the following commands:

\begin{verbatim}
SELECT * FROM posts WHERE PostTypeId=1 AND CreationDate > '20110813'
    AND ClosedDate > '20110813';

SELECT * FROM posts Where PostTypeId=1 AND CreatedDate > '20110813'
    AND ClosedDate IS NULL;
\end{verbatim}

These two queries aim to retrieve all questions (PostTypeId=1 represents a question, as opposed to a comment or an answer) posted on StackOverflow on or after August 13$^{\textrm{th}}$, 2011 that: 1)
were closed on or after August 13$^{\textrm{th}}$ and 2) have not been
closed, respectively.  However, to prevent overloading the StackExchange servers, the Data
Explorer will not return more than 50,000 rows at a time.  Thus, after
issuing these two queries, we had collected a data set of 50,000
questions that had been closed (55MB) and 50,000 that were still open as of
query issue date (75MB).

The data was formatted as CSV files and contained the following
fields: Question Id, PostTypeId, AcceptedAnswerId, ParentId,
CreationDate, Score, ViewCount, Body, OwnerUserId, LastEditorUserId,
LastEditorDisplayName, LastEditDate, LastActivityData, Title, Tags,
AnswerCount, CommentCount, FavoriteCount, ClosedDate and
CommunityOwnedDate. Of these fields, the fields that we worked with
were in our initial testing were:

\begin{enumerate}
  \item Body - the raw text of the question
  \item Tags - user provided question categories (e.g. Java,
    algorithms, etc.)
\end{enumerate}

Midway through our experiments, we discovered a \emph{Kaggle}
competition for predicting closed questions on \emph{Stackoverflow}
\cite{website:kaggle}. The competition materials included larger data sets: a
sample train set (133MB) and a larger train set (3.5G).  These data
files were also formatted as CSV files however included different
question fields: PostId, PostCreationDate, OwnerUserId,
OwnerCreationDate, ReutationAtPostCreation,
OwnerUndeletedAnswerCountAtPostTime, Title, BodyMarkdown, Tag1, Tag2,
Tag3, Tag4, Tag5, PostClosedDate and OpenStatus.

\subsection{Pre-processing}
To get the data in workable form, a number of pre-processing steps
were required. First, the ``body'' field, which contained the text of
each question, was formatted as HTML and had to be
cleaned (i.e. HTML tags stripped). Additionally, images and code were
removed.  We had to remove code because it is highly variable
(variables always have different names) and it contains a number of
strange characters and formatting.

Additionally, there were a few components of a question whose presence
usually indicated that the question would be closed.  For example, the
body of a question can include a \begin{verbatim}
  <strong> \end{verbatim} tag which indicates that a Stackoverflow
moderator (who have the power to close posts) has commented on the
question. These tags showed a large positive correlation to closed
questions and thus, needed to be removed. Similarly, all ``Possible
duplicate'', ``Closed'', or other moderator edits to a question, needed to be
removed (as clearly ).  Finally, any urls, which usually hyperlinked to a similar
question on Stackoverflow, were removed.

After being cleaned, the text of each question was tokenized.

\subsection{Features}

For each question, two categories of features were extracted and used
for training: features extracted from the body of the question and
features extracted concerning the metadata of that particular
question. As metadata features, the following fields were extracted:

\begin{itemize}
  \item Bucketed question length (in characters); a binary feature
    taking the value '1' if the questions has more than 1000 characters
  \item Question tags; converted to a list of strings and concatenated
    onto each feature vector
\end{itemize}

Before extracting features from question bodies, first, each question
body was down-cased and tokenized. Then, the count of each unigram, bigram and
trigram, per question, was added to each feature vector. Finally, the
cross product of all tags and distinct unigrams, per question, was
added to each feature vector.  Using both the metadata and ngram
features, each feature vector was on the order of 2.5 million features
long.

\section{Software}

To facilitate writing code and expediting experimentation, we made
extensive use of third party software. In the pre-processing stage, we
used two different libraries.  First, to parse the CSV files
collected, we used a CSV parser called SuperCSV, written in Java
\cite{website:supercsv}. In addition, to tokenize the question bodies,
we made use of the Stanford NLP Library, also written in Java
\cite{stanfordnlp}.

To facilitate experimentation with various classifiers, we used the
FACTORIE library, written in Scala, that is developed at UMass
Amherst.  This library provides implementations of a variety of
textual pre-processing functions and machine learning classifiers
\cite{mccallum09:factorie:}.

\section{Methods}
\begin{itemize}
\item using factorie facilities for storing instances, trimming domains, learning classifiers, judging accuracy, info gain, etc. Need sparse vectors cause lots of data. pretty fast sgd implementations
\item Use information gain to see suspicious features
\end{itemize}

\section{Results}
\begin{itemize}
\item 71\% accuracy with logistic regression trained using AdaGrad (not l2 regularized but effectively regularized since its an online learning algorithm). SVM, naive bayes, l2 log reg etc does slightly worse.
\item Naive bayes was winner until we added cross product
  features. High bias/low variance does better
  \item How long did it take run the whole thing?
\end{itemize}

\begin{figure}
\centering
\fbox{\includegraphics[width=6.5in,height=6in]{images/stackoverflow_results}}
\caption{Classifier accuracies in predicting closed questions}
\label{fig:results}
\end{figure}

\section{Discussion}

\begin{itemize}
  \item Ngrams didn’t help - need smarter features - we get 100\%
    almost train set accuracy, so the problem is bad features, not
    inability to fit. Regularization also doesn’t help much at all
    (tried manually grid searching l2 lambda). Overfitting is not the
    problem.
  \item using information gain to figure out which features were best
  \item What where the best features? Why were they best?
  \item What was the best classifier, why?
\end{itemize}

\section{Future directions}
\begin{itemize}
\item Use kaggle data set (which we have); 70 million training instances, 80 thousand test instances
\item We’ll need to split up the data (because it’s big) to be able to use it; we’re planning to learn the weights of our linear classifier using stochastic gradient descent (blocked-adagrad)
\item Add more features (TFIDF, other NLP features), heuristic of adding number of rare words from Alex’s paper
\item Try out new classifiers - random forests
\item Try things besides open/closed - like answered/unanswered
\end{itemize}

\bibliography{bib}{}
\bibliographystyle{plain}

\end{document}
