\documentclass[11pt]{article}

\usepackage{graphicx, amsmath, amsfonts, amssymb, microtype, fullpage, url, algorithm, algorithmic, hyperref, longtable}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\setlength{\parskip}{5pt}

\title{Project Report - Classification of questions on stackoverflow\\ Machine Learning, Fall 2012}
\author{Luke Vilnis, Annamalai Natarajan, Ariel Kobren}

\begin{document}
\sloppy

\maketitle
\tableofcontents
\pagebreak

\section{Problem Statement}
\begin{itemize}
\item What is a closed question?
\item Why would a question be closed?
	\begin{itemize}
	\item Exact duplicate - dropping closed questions in this category
	\item Off-topic
	\item Not constructive
	\item Note a real question
	\item Too localized
	\end{itemize}
\item What are the challenges in this problem?
	\begin{itemize}
	\item Finding closed questions
	\item Sizeable chunk of closed question is code (which is stripped off)
	\item Class label imbalance
	\end{itemize}
\end{itemize}

\section{Data set}
In our initial work, we used a data set collected using
Stackoverflow's \emph{StackExchange Data Explorer}
\cite{website:stackexchange}. This online tool
allows users to query raw data directly from
Stackoverflow. Specifically, we issued the following commands:

\begin{verbatim}
SELECT * FROM posts WHERE PostTypeId=1 AND CreationDate > '20110813'
    AND ClosedDate > '20110813';

SELECT * FROM posts Where PostTypeId=1 AND CreatedDate > '20110813'
    AND ClosedDate IS NULL;
\end{verbatim}

These two queries aim to retrieve all questions (PostTypeId=1) posted
on Stackoverflow on or after August 13$^{\textrm{th}}$, 2011 that: 1)
were closed on or after August 13$^{\textrm{th}}$ and 2) have not been
closed, respectively.  However, as a matter of policy, the Data
Explorer will not return more than 50,000 rows at a time.  Thus, after
issuing these two queries, we had collected a data set of 50,000
questions that had been closed (55MB) and 50,000 that were still open as of
query issue date (75MB).

The data was formatted as CSV files and contained the following
fields: Question Id, PostTypeId, AcceptedAnswerId, ParentId,
CreationDate, Score, ViewCount, Body, OwnerUserId, LastEditorUserId,
LastEditorDisplayName, LastEditDate, LastActivityData, Title, Tags,
AnswerCount, CommentCount, FavoriteCount, ClosedDate and
CommunityOwnedDate. Of these fields, the fields that we worked with
were in our initial testing were:

\begin{enumerate}
  \item Body - the raw text of the question
  \item Tags - user provided question categories (e.g. Java,
    algorithms, etc.)
\end{enumerate}

To parse the CSV files collected, we used third party CSV Praser from
SuperCSV, written in Java \cite{website:supercsv}.

Midway through our experiments, we discovered a \emph{Kaggle}
competition for predicting closed questions on \emph{Stackoverflow}
\cite{kaggle}. The competition materials included larger data sets: a
sample train set (133MB) and a larger train set (3.5G).  These data
files were also formatted as CSV files however included different
question fields: PostId, PostCreationDate, OwnerUserId,
OwnerCreationDate, ReutationAtPostCreation,
OwnerUndeletedAnswerCountAtPostTime, Title, BodyMarkdown, Tag1, Tag2,
Tag3, Tag4, Tag5, PostClosedDate and OpenStatus.

\subsection{Pre-processing}
To get the data in workable form, a number of pre-processing steps
were required. First, the ``body'' field, which contained the text of
each question, was formatted as HTML and had to be
cleaned (i.e. HTML tags stripped). Additionally, images and code were
removed.  We had to remove code because it is highly variable
(variables always have different names) and it contains a number of
strange characters and formatting.

Additionally, there were a few components of a question whose presence
usually indicated that the question would be closed.  For example, the
body of a question can include a \begin{verbatim}
  <strong> \end{verbatim} tag which indicates that a Stackoverflow
moderator (who have the power to close posts) has commented on the
question. These tags showed a large positive correlation to closed
questions and thus, needed to be removed. Similarly, all ``Possible
duplicate'' or ``Closed'' notes, attached to a question, needed to be
removed.  Finally, any urls, which usually hyperlinked to a similar
question on Stackoverflow, were removed.

After being cleaned, the text of each question was tokenized.

\subsection{Features}

\begin{itemize}
\item How did we extract features from questions?
\item bucketed question length
\item presence of html tags (blockquote, code, url, etc)
\item question tags and cross products of those tags with word features
\item Stanford tokenizer
\item Ngrams didn’t help - need smarter features - we get 100\% almost train set accuracy, so the problem is bad features, not inability to fit. Regularization also doesn’t help much at all (tried manually grid searching l2 lambda). Overfitting is not the problem.
\end{itemize}

\section{Methods}
\begin{itemize}
\item using factorie facilities for storing instances, trimming domains, learning classifiers, judging accuracy, info gain, etc. Need sparse vectors cause lots of data. pretty fast sgd implementations
\item Use information gain to see suspicious features
\end{itemize}

\section{Results}
\begin{itemize}
\item 71\% accuracy with logistic regression trained using AdaGrad (not l2 regularized but effectively regularized since its an online learning algorithm). SVM, naive bayes, l2 log reg etc does slightly worse.
\item Naive bayes was winner until we added cross product features. High bias/low variance does better
\end{itemize}

\begin{figure}
\centering
\fbox{\includegraphics[width=6.5in,height=6in]{stackoverflow_results}}
\caption{Classifier accuracies in predicting closed questions}
\label{fig:results}
\end{figure}

\section{Future directions}
\begin{itemize}
\item Use kaggle data set (which we have); 70 million training instances, 80 thousand test instances
\item We’ll need to split up the data (because it’s big) to be able to use it; we’re planning to learn the weights of our linear classifier using stochastic gradient descent (blocked-adagrad)
\item Add more features (TFIDF, other NLP features), heuristic of adding number of rare words from Alex’s paper
\item Try out new classifiers - random forests
\item Try things besides open/closed - like answered/unanswered
\end{itemize}

\bibliography{bib}{}
\bibliographystyle{plain}

\end{document}
